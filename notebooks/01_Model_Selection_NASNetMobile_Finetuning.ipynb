{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64eece7d",
   "metadata": {},
   "source": [
    "# Tensorflow Image Classifier\n",
    "\n",
    "## NASNetMobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd146210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay)\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.io import TFRecordWriter\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks  import (\n",
    "    Callback,\n",
    "    CSVLogger,\n",
    "    EarlyStopping,\n",
    "    LearningRateScheduler,\n",
    "    ModelCheckpoint\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Layer,\n",
    "    GlobalAveragePooling2D,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    InputLayer,\n",
    "    BatchNormalization,\n",
    "    Input,\n",
    "    Dropout,\n",
    "    RandomFlip,\n",
    "    RandomRotation,\n",
    "    RandomContrast,\n",
    "    RandomBrightness,\n",
    "    Resizing,\n",
    "    Rescaling\n",
    ")\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import L2, L1\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.train import Feature, Features, Example, BytesList, Int64List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 32\n",
    "SIZE = 224\n",
    "SEED = 42\n",
    "\n",
    "EPOCHS = 20\n",
    "LR = 3e-4\n",
    "FILTERS = 6\n",
    "KERNEL = 3\n",
    "STRIDES = 1\n",
    "REGRATE = 0.0\n",
    "POOL = 2\n",
    "DORATE = 0.05\n",
    "LABELS = ['Gladiolus', 'Adenium', 'Alpinia_Purpurata', 'Alstroemeria', 'Amaryllis', 'Anthurium_Andraeanum', 'Antirrhinum', 'Aquilegia', 'Billbergia_Pyramidalis', 'Cattleya', 'Cirsium', 'Coccinia_Grandis', 'Crocus', 'Cyclamen', 'Dahlia', 'Datura_Metel', 'Dianthus_Barbatus', 'Digitalis', 'Echinacea_Purpurea', 'Echinops_Bannaticus', 'Fritillaria_Meleagris', 'Gaura', 'Gazania', 'Gerbera', 'Guzmania', 'Helianthus_Annuus', 'Iris_Pseudacorus', 'Leucanthemum', 'Malvaceae', 'Narcissus_Pseudonarcissus', 'Nerine', 'Nymphaea_Tetragona', 'Paphiopedilum', 'Passiflora', 'Pelargonium', 'Petunia', 'Platycodon_Grandiflorus', 'Plumeria', 'Poinsettia', 'Primula', 'Protea_Cynaroides', 'Rose', 'Rudbeckia', 'Strelitzia_Reginae', 'Tropaeolum_Majus', 'Tussilago', 'Viola', 'Zantedeschia_Aethiopica']\n",
    "NLABELS = len(LABELS)\n",
    "DENSE1 = 128\n",
    "DENSE2 = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aca3ff",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7436d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = '../dataset/Flower_Dataset/split/train'\n",
    "test_directory = '../dataset/Flower_Dataset/split/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7bb62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = image_dataset_from_directory(\n",
    "    train_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=LABELS,\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH,\n",
    "    image_size=(SIZE, SIZE),\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "\n",
    "# Found 9206 files belonging to 48 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = image_dataset_from_directory(\n",
    "    test_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=LABELS,\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH,\n",
    "    image_size=(SIZE, SIZE),\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Found 3090 files belonging to 48 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1958f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = Sequential([\n",
    "        # Resizing(256, 256),\n",
    "        RandomRotation(factor=0.15),\n",
    "        RandomFlip(),\n",
    "        RandomContrast(factor=0.1),\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = (\n",
    "    train_dataset\n",
    "    .map(lambda image, label: (data_augmentation(image), label))\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n",
    "testing_dataset = (\n",
    "    test_dataset.prefetch(\n",
    "        tf.data.AUTOTUNE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476bc53",
   "metadata": {},
   "source": [
    "## Building the NASNet TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4647809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning\n",
    "\n",
    "backbone = tf.keras.applications.NASNetMobile(\n",
    "    input_shape=(SIZE, SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b52576-f119-422d-9283-e35331d09742",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.trainable = False\n",
    "\n",
    "# Unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
    "for layer in backbone.layers[-20:]:\n",
    "    if not isinstance(layer, BatchNormalization):\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51fe5f-4fba-4a8b-a7db-7198e5db6d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(SIZE,SIZE,3))\n",
    "x = data_augmentation(input)\n",
    "x = backbone(x, training=False)\n",
    "\n",
    "# Rebuild non-frozen top layers of NASNetMobile, which was initialized with include_top=False\n",
    "x = GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(DENSE1, activation=\"relu\")(x)\n",
    "x = Dense(DENSE2, activation=\"relu\")(x)\n",
    "output = Dense(NLABELS, activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "nasnet_model = Model(input, output, name=\"NASNetMobile\")\n",
    "nasnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    '../best_weights',\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf3afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804c2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [CategoricalAccuracy(name='accuracy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ba578",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasnet_model.compile(\n",
    "    optimizer = Adam(learning_rate=LR),\n",
    "    loss = loss_function,\n",
    "    metrics = metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c2abe",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b5602",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nasnet_history = nasnet_model.fit(\n",
    "    training_dataset,\n",
    "    validation_data = testing_dataset,\n",
    "    epochs = EPOCHS,\n",
    "    verbose = 1,\n",
    "    # callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")\n",
    "\n",
    "# loss: 2.9544\n",
    "# accuracy: 0.2090\n",
    "# val_loss: 3.2046\n",
    "# val_accuracy: 0.1838"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995bba2c",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasnet_model.evaluate(testing_dataset)\n",
    "# loss: 3.2046 - accuracy: 0.1838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759dede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(nasnet_history.history['loss'])\n",
    "plt.plot(nasnet_history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.savefig('assets/NASNetMobile_FT_01.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf95e5e",
   "metadata": {},
   "source": [
    "![tf Emotion Detection](./assets/NASNetMobile_FT_01.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5733e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(nasnet_history.history['accuracy'])\n",
    "plt.plot(nasnet_history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'])\n",
    "plt.savefig('assets/NASNetMobile_FT_02.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503a917",
   "metadata": {},
   "source": [
    "![tf Emotion Detection](./assets/NASNetMobile_FT_02.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1535b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_image_bgr = cv.imread('../dataset/snapshots/Viola_Tricolor.jpg')\n",
    "test_image_resized = cv.resize(test_image_bgr, (SIZE, SIZE))\n",
    "test_image_rgb = cv.cvtColor(test_image_resized, cv.COLOR_BGR2RGB)\n",
    "img = tf.constant(test_image_rgb, dtype=tf.float32)\n",
    "img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "probs = nasnet_model(img).numpy()\n",
    "label = LABELS[tf.argmax(probs, axis=1).numpy()[0]]\n",
    "\n",
    "print(label, str(probs[0]))\n",
    "\n",
    "plt.imshow(test_image_rgb)\n",
    "plt.title(label)\n",
    "plt.axis('off')\n",
    "        \n",
    "plt.savefig('assets/NASNetMobile_Prediction_01.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc255bb-a3d6-48e7-8e54-f4fdcbd3f8fe",
   "metadata": {},
   "source": [
    "![tf Emotion Detection](./assets/NASNetMobile_Prediction_01.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b7232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_image_bgr = cv.imread('../dataset/snapshots/Strelitzia.jpg')\n",
    "test_image_resized = cv.resize(test_image_bgr, (SIZE, SIZE))\n",
    "test_image_rgb = cv.cvtColor(test_image_resized, cv.COLOR_BGR2RGB)\n",
    "img = tf.constant(test_image_rgb, dtype=tf.float32)\n",
    "img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "probs = nasnet_model(img).numpy()\n",
    "label = LABELS[tf.argmax(probs, axis=1).numpy()[0]]\n",
    "\n",
    "print(label, str(probs[0]))\n",
    "\n",
    "plt.imshow(test_image_rgb)\n",
    "plt.title(label)\n",
    "plt.axis('off')\n",
    "        \n",
    "plt.savefig('assets/NASNetMobile_Prediction_02.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7675e-efdf-47fe-968a-8119287500fc",
   "metadata": {},
   "source": [
    "![tf Emotion Detection](./assets/NASNetMobile_Prediction_02.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06be8a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_image_bgr = cv.imread('../dataset/snapshots/Water_Lilly.jpg')\n",
    "test_image_resized = cv.resize(test_image_bgr, (SIZE, SIZE))\n",
    "test_image_rgb = cv.cvtColor(test_image_resized, cv.COLOR_BGR2RGB)\n",
    "img = tf.constant(test_image_rgb, dtype=tf.float32)\n",
    "img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "probs = nasnet_model(img).numpy()\n",
    "label = LABELS[tf.argmax(probs, axis=1).numpy()[0]]\n",
    "\n",
    "print(label, str(probs[0]))\n",
    "\n",
    "plt.imshow(test_image_rgb)\n",
    "plt.title(label)\n",
    "plt.axis('off')\n",
    "        \n",
    "plt.savefig('assets/NASNetMobile_Prediction_03.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d89a8-a0d6-4992-a94a-12dfd201b395",
   "metadata": {},
   "source": [
    "![tf Emotion Detection](./assets/NASNetMobile_Prediction_03.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354301e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "for images, labels in testing_dataset.take(1):\n",
    "    for i in range(16):\n",
    "        ax = plt.subplot(4,4,i+1)\n",
    "        true = \"True: \" + LABELS[tf.argmax(labels[i], axis=0).numpy()]\n",
    "        pred = \"Predicted: \" + LABELS[\n",
    "            tf.argmax(nasnet_model(tf.expand_dims(images[i], axis=0)).numpy(), axis=1).numpy()[0]\n",
    "        ]\n",
    "        plt.title(\n",
    "           true  + \"\\n\" + pred\n",
    "        )\n",
    "        plt.imshow(images[i]/255.)\n",
    "        plt.axis('off')\n",
    "        \n",
    "plt.savefig('assets/NASNetMobile_FT_03.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1683278",
   "metadata": {},
   "source": [
    "![tf Emotion Detection](./assets/NASNetMobile_FT_03.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d778a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_test = []\n",
    "\n",
    "for img, label in testing_dataset:\n",
    "    y_pred.append(nasnet_model(img))\n",
    "    y_test.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ae3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_mtx = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        np.argmax(y_test[:-1], axis=-1).flatten(),\n",
    "        np.argmax(y_pred[:-1], axis=-1).flatten()\n",
    "    ),\n",
    "    display_labels=LABELS\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "conf_mtx.plot(ax=ax, cmap='plasma', include_values=True, xticks_rotation='vertical',)\n",
    "\n",
    "plt.savefig('assets/NASNetMobile_FT_04.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ec2e3",
   "metadata": {},
   "source": [
    "![tf Emotion Detection](./assets/NASNetMobile_FT_04.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c782363",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.saving.save_model(\n",
    "    nasnet_model, '../saved_model/nasnetmobile_model_ft/1', overwrite=True, save_format='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60163650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore the model\n",
    "restored_model2 = tf.keras.saving.load_model('../saved_model/nasnetmobile_model_ft/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check its architecture\n",
    "restored_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_model2.evaluate(testing_dataset)\n",
    "# loss: 3.2046 - accuracy: 0.1838"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

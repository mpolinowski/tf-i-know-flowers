{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643e3192",
   "metadata": {},
   "source": [
    "## DeiT Vision Transformer (Transfer-Learning) \n",
    "\n",
    "> [Training data-efficient image transformers\n",
    "& distillation through attention](https://arxiv.org/pdf/2012.12877.pdf)\n",
    "> \n",
    "> `Hugo Touvron`, `Matthieu Cord`, `Matthijs Douze`, `Francisco Massa`, `Alexandre Sablayrolles`, `Herve J'egou`\n",
    "\n",
    "\n",
    "_\"Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These highperforming vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5cc00f-82d7-43f1-8635-dcb0dc417bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11c094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    BatchNormalization,\n",
    "    LayerNormalization,\n",
    "    Dense,\n",
    "    Input,\n",
    "    Embedding,\n",
    "    MultiHeadAttention,\n",
    "    Layer,\n",
    "    Add,\n",
    "    Resizing,\n",
    "    Rescaling,\n",
    "    Permute,\n",
    "    Flatten,\n",
    "    RandomFlip,\n",
    "    RandomRotation,\n",
    "    RandomContrast,\n",
    "    RandomBrightness\n",
    ")\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, TopKCategoricalAccuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "from transformers import DeiTConfig, TFDeiTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "LABELS = ['Gladiolus', 'Adenium', 'Alpinia_Purpurata', 'Alstroemeria', 'Amaryllis', 'Anthurium_Andraeanum', 'Antirrhinum', 'Aquilegia', 'Billbergia_Pyramidalis', 'Cattleya', 'Cirsium', 'Coccinia_Grandis', 'Crocus', 'Cyclamen', 'Dahlia', 'Datura_Metel', 'Dianthus_Barbatus', 'Digitalis', 'Echinacea_Purpurea', 'Echinops_Bannaticus', 'Fritillaria_Meleagris', 'Gaura', 'Gazania', 'Gerbera', 'Guzmania', 'Helianthus_Annuus', 'Iris_Pseudacorus', 'Leucanthemum', 'Malvaceae', 'Narcissus_Pseudonarcissus', 'Nerine', 'Nymphaea_Tetragona', 'Paphiopedilum', 'Passiflora', 'Pelargonium', 'Petunia', 'Platycodon_Grandiflorus', 'Plumeria', 'Poinsettia', 'Primula', 'Protea_Cynaroides', 'Rose', 'Rudbeckia', 'Strelitzia_Reginae', 'Tropaeolum_Majus', 'Tussilago', 'Viola', 'Zantedeschia_Aethiopica']\n",
    "NLABELS = len(LABELS)\n",
    "BATCH_SIZE = 32\n",
    "SIZE = 224\n",
    "EPOCHS = 40\n",
    "LR = 5e-6 # default 0.001\n",
    "HIDDEN_SIZE = 768 # default 768\n",
    "NHEADS = 8 # default 12\n",
    "NLAYERS = 4 # default 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef778cd",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f753146",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory = '../dataset/Flower_Dataset/split/train'\n",
    "test_directory = '../dataset/Flower_Dataset/split/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed88828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = image_dataset_from_directory(\n",
    "    train_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=LABELS,\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(SIZE, SIZE),\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "\n",
    "# Found 9206 files belonging to 48 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dda8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = image_dataset_from_directory(\n",
    "    test_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=LABELS,\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(SIZE, SIZE),\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Found 3090 files belonging to 48 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b670460",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = Sequential([\n",
    "        RandomRotation(factor=0.25),\n",
    "        RandomFlip(mode='horizontal'),\n",
    "        RandomContrast(factor=0.1),\n",
    "        RandomBrightness(0.1)\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7921c41-8ff6-4485-a71f-5c7ed6c92a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_rescale_reshape = Sequential([\n",
    "    Resizing(SIZE, SIZE),\n",
    "    Rescaling(1./255),\n",
    "    # transformer expects image shape (3,224,224)\n",
    "    Permute((3,1,2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = (\n",
    "    train_dataset\n",
    "    .map(lambda image, label: (data_augmentation(image), label))\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n",
    "testing_dataset = (\n",
    "    test_dataset.prefetch(\n",
    "        tf.data.AUTOTUNE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f3770b",
   "metadata": {},
   "source": [
    "### DeiT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec31f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing a ViT vit-base-patch16-224 style configuration\n",
    "configuration = DeiTConfig(\n",
    "    image_size=SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_attention_heads=NHEADS,\n",
    "    num_hidden_layers=NLAYERS\n",
    ")\n",
    "\n",
    "# Initializing a model with random weights from the vit-base-patch16-224 style configuration\n",
    "# base_model = TFViTModel(configuration)\n",
    "\n",
    "# use pretrained weights for the model instead\n",
    "base_model = TFDeiTModel.from_pretrained(\"facebook/deit-base-distilled-patch16-224\", config=configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = base_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e44112-7d14-41db-9c0a-15162f2a5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94afcd8-ca1a-490d-9e22-9e1c6d43f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(SIZE,SIZE,3))\n",
    "# random image augmentation\n",
    "data_aug = data_augmentation(input)\n",
    "x = resize_rescale_reshape(data_aug)\n",
    "x = base_model.deit(x)[0][:,0,:]\n",
    "output = Dense(NLABELS, activation='softmax')(x)\n",
    "\n",
    "deit_model = Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf81dd-f624-4ba5-b254-17b43c87a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deit_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc4f3f-a440-40e3-8e11-a2228df28d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the pretrained model\n",
    "test_image = cv.imread('../dataset/snapshots/Viola_Tricolor.jpg')\n",
    "test_image = cv.resize(test_image, (SIZE, SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536e3b1-56d4-4ef3-8b25-a1555f06ee19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deit_model(tf.expand_dims(test_image, axis = 0))\n",
    "# numpy= array([[1.0963462e-01, 4.4628163e-03, 2.7227099e-03, 3.9012067e-02,\n",
    "      #   1.2207581e-02, 3.4460202e-02, 2.3577355e-03, 3.5261197e-03,\n",
    "      #   1.7803181e-02, 1.0567555e-02, 1.5943516e-02, 4.0797489e-03,\n",
    "      #   7.1987398e-03, 9.5541059e-04, 4.2675242e-02, 1.5655500e-04,\n",
    "      #   1.1215543e-02, 1.4889235e-02, 1.8372904e-01, 7.0088580e-03,\n",
    "      #   3.1637046e-03, 1.4315472e-03, 8.3367303e-03, 1.5427665e-03,\n",
    "      #   1.9941023e-02, 9.9778855e-03, 5.6907861e-03, 1.7462631e-03,\n",
    "      #   3.6991950e-02, 1.3322993e-02, 5.4029688e-02, 4.0368687e-02,\n",
    "      #   6.1121010e-03, 7.9112053e-03, 7.2245464e-02, 8.8621033e-03,\n",
    "      #   2.1858371e-03, 3.0036021e-02, 2.7811823e-02, 7.0134280e-03,\n",
    "      #   6.1850133e-03, 1.8044524e-02, 2.3036957e-02, 1.6069075e-02,\n",
    "      #   2.3161862e-02, 2.9986592e-03, 1.0242336e-02, 1.6933089e-02]],\n",
    "      # dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec69a58",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc980d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a30969",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [CategoricalAccuracy(name='accuracy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deit_model.compile(\n",
    "    optimizer = Adam(learning_rate = LR),\n",
    "    loss = loss_function,\n",
    "    metrics = metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786148c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deit_history = deit_model.fit(\n",
    "    training_dataset,\n",
    "    validation_data = testing_dataset,\n",
    "    epochs = EPOCHS,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "# loss: 0.3592\n",
    "# accuracy: 0.8945\n",
    "# val_loss: 0.7199\n",
    "# val_accuracy: 0.7900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd0073f-7694-48bc-b90c-7f515b47750e",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c682a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deit_model.evaluate(testing_dataset)\n",
    "# loss: 0.7199 - accuracy: 0.7900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bffe28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(deit_history.history['loss'])\n",
    "plt.plot(deit_history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "\n",
    "plt.savefig('assets/DeiT_01.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0419a3",
   "metadata": {},
   "source": [
    "![Building a Tensorflow VIT](./assets/DeiT_01.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc6089b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(deit_history.history['accuracy'])\n",
    "plt.plot(deit_history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_accuracy', 'val_accuracy'])\n",
    "\n",
    "plt.savefig('assets/DeiT_02.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc766d",
   "metadata": {},
   "source": [
    "![Building a Tensorflow VIT](./assets/DeiT_02.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd2369-7042-42e3-912c-6915095271fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_image_bgr = cv.imread('../dataset/snapshots/Viola_Tricolor.jpg')\n",
    "test_image_resized = cv.resize(test_image_bgr, (SIZE, SIZE))\n",
    "test_image_rgb = cv.cvtColor(test_image_resized, cv.COLOR_BGR2RGB)\n",
    "img = tf.constant(test_image_rgb, dtype=tf.float32)\n",
    "img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "probs = deit_model(img).numpy()\n",
    "label = LABELS[tf.argmax(probs, axis=1).numpy()[0]]\n",
    "\n",
    "print(label, str(probs[0]))\n",
    "\n",
    "plt.imshow(test_image_rgb)\n",
    "plt.title(label)\n",
    "plt.axis('off')\n",
    "        \n",
    "plt.savefig('assets/DeiT_Prediction_01.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79133144-e26c-4079-b831-8df966d4527e",
   "metadata": {},
   "source": [
    "![tf Emotion Detection](./assets/DeiT_Prediction_01.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f4b45-4f40-47a5-b093-27dc3e7977d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_image_bgr = cv.imread('../dataset/snapshots/Strelitzia.jpg')\n",
    "test_image_resized = cv.resize(test_image_bgr, (SIZE, SIZE))\n",
    "test_image_rgb = cv.cvtColor(test_image_resized, cv.COLOR_BGR2RGB)\n",
    "img = tf.constant(test_image_rgb, dtype=tf.float32)\n",
    "img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "probs = deit_model(img).numpy()\n",
    "label = LABELS[tf.argmax(probs, axis=1).numpy()[0]]\n",
    "\n",
    "print(label, str(probs[0]))\n",
    "\n",
    "plt.imshow(test_image_rgb)\n",
    "plt.title(label)\n",
    "plt.axis('off')\n",
    "        \n",
    "plt.savefig('assets/DeiT_Prediction_02.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e75a04-4afe-4a96-b1eb-12684f5c73db",
   "metadata": {},
   "source": [
    "![tf Emotion Detection](./assets/DeiT_Prediction_02.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae51b06-c42e-4588-8826-6089b78a4d6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_image_bgr = cv.imread('../dataset/snapshots/Water_Lilly.jpg')\n",
    "test_image_resized = cv.resize(test_image_bgr, (SIZE, SIZE))\n",
    "test_image_rgb = cv.cvtColor(test_image_resized, cv.COLOR_BGR2RGB)\n",
    "img = tf.constant(test_image_rgb, dtype=tf.float32)\n",
    "img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "probs = deit_model(img).numpy()\n",
    "label = LABELS[tf.argmax(probs, axis=1).numpy()[0]]\n",
    "\n",
    "print(label, str(probs[0]))\n",
    "\n",
    "plt.imshow(test_image_rgb)\n",
    "plt.title(label)\n",
    "plt.axis('off')\n",
    "        \n",
    "plt.savefig('assets/DeiT_Prediction_03.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aabd4c-5829-4743-babe-2a14d4e7d8a9",
   "metadata": {},
   "source": [
    "![tf Emotion Detection](./assets/DeiT_Prediction_03.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd57ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "for images, labels in testing_dataset.take(1):\n",
    "    for i in range(16):\n",
    "        ax = plt.subplot(4,4,i+1)\n",
    "        true = \"True: \" + LABELS[tf.argmax(labels[i], axis=0).numpy()]\n",
    "        pred = \"Predicted: \" + LABELS[\n",
    "            tf.argmax(deit_model(tf.expand_dims(images[i], axis=0)).numpy(), axis=1).numpy()[0]\n",
    "        ]\n",
    "        plt.title(\n",
    "           true  + \"\\n\" + pred\n",
    "        )\n",
    "        plt.imshow(images[i]/255.)\n",
    "        plt.axis('off')\n",
    "        \n",
    "plt.savefig('assets/DeiT_03.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9856ee",
   "metadata": {},
   "source": [
    "![Building a Tensorflow VIT](./assets/DeiT_03.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70875ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_test = []\n",
    "\n",
    "for img, label in testing_dataset:\n",
    "    y_pred.append(deit_model(img))\n",
    "    y_test.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527da3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_mtx = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(\n",
    "        np.argmax(y_test[:-1], axis=-1).flatten(),\n",
    "        np.argmax(y_pred[:-1], axis=-1).flatten()\n",
    "    ),\n",
    "    display_labels=LABELS\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,12))\n",
    "conf_mtx.plot(ax=ax, cmap='plasma', include_values=True, xticks_rotation='vertical')\n",
    "\n",
    "plt.savefig('assets/DeiT_04.webp', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab425e",
   "metadata": {},
   "source": [
    "![Building a Tensorflow VIT](./assets/DeiT_04.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f6101-398b-47fb-aa0e-092aeb91b960",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14197596-ec89-4ec3-acb2-ebd6b5422035",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.saving.save_model(\n",
    "    deit_model, '../saved_model/deit_model/1', overwrite=True, save_format='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c112172-5293-4c37-ad59-497aca441019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore the model\n",
    "restored_model = tf.keras.saving.load_model('../saved_model/deit_model/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18329b18-bae8-40f7-aad1-b70c3df5f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check its architecture\n",
    "restored_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c79d4-9759-445b-acda-a0f027c018dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_model.evaluate(testing_dataset)\n",
    "# loss: 0.5184 - accuracy: 0.7840 - topk_accuracy: 0.9394"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
